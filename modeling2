
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from collections import deque
import tensorflow as tf
from datetime import datetime
import psycopg2
import redis

# 딥러닝 모델 로드
model = tf.keras.models.load_model("C:\\Users\\Jsystem\\.cache\\huggingface\\hub\\models--keras-io--timeseries-anomaly-detection\\snapshots\\57126012e02dcab3a653501f12cb599c4a51db7a")


# 데이터베이스 연결 설정
conn = psycopg2.connect(
    dbname="postgres",
    user="postgres",
    password="1234",
    host="localhost",
    port="5432"
)
cursor = conn.cursor()

# Redis 연결 설정
client = redis.from_url('redis://localhost')
pipeline = client.pipeline()
stream_name = 'sensorDataStream'

# 데이터 정규화 스케일러
scaler = MinMaxScaler()

# 시계열 데이터 큐 초기화 (시퀀스 길이만큼 유지)
sequence_length = 288
data_queue = deque(maxlen=sequence_length)

# Redis 및 데이터베이스에 저장 함수
def save_to_db_and_redis(sensor_id, time, value, prediction, is_outlier):
    data = (sensor_id, time, value, prediction, is_outlier)
    
    # 데이터베이스에 저장
    try:
        cursor.execute("""
            INSERT INTO vibration (sensor_id, time, value, prediction, outlier_status)
            VALUES (%s, %s, %s, %s, %s)
        """, data)
        conn.commit()
    except Exception as e:
        print(f"Database insertion error: {e}")
        conn.rollback()

    # Redis Stream 저장
    stream_data = {
        "sensorId": sensor_id,
        "time": str(time),
        "value": value,
        "prediction": prediction,
        "outlierStatus": str(is_outlier)
    }
    pipeline.xadd(stream_name, stream_data, maxlen=2000)
    pipeline.execute()

# 이상치 탐지 함수
def detect_outlier_with_model(new_value):
    # 데이터 정규화 및 큐에 추가
    normalized_value = scaler.transform([[new_value]])[0][0]
    data_queue.append(normalized_value)
    
    # 시퀀스가 충분히 쌓인 경우
    if len(data_queue) == sequence_length:
        input_sequence = np.array(data_queue).reshape(1, sequence_length, 1)
        prediction = model.predict(input_sequence)[0][0]
        
        # 이상치 판단 (임계값 설정 가능)
        threshold = 0.1
        is_outlier = abs(new_value - prediction) > threshold
        return prediction, is_outlier
    return None, None

# 실시간 데이터 처리
def process_real_time_data(sensor_id, raw_value):
    current_time = datetime.now()
    
    # 모델 기반 이상치 탐지
    prediction, is_outlier = detect_outlier_with_model(raw_value)
    
    # 결과 저장
    if prediction is not None:
        save_to_db_and_redis(sensor_id, current_time, raw_value, prediction, is_outlier)

# 예시: 센서 데이터 처리 루프
try:
    while True:
        # 센서 데이터 수신 (여기서는 임의의 값 사용)
        sensor_id = 1  # 온도 센서 ID
        raw_value = np.random.uniform(0, 100)  # 센서에서 수신한 값

        # 데이터 처리
        process_real_time_data(sensor_id, raw_value)

except KeyboardInterrupt:
    print("Stopping real-time data processing...")
    conn.close()


###################################################

## 버전 확인 (hugging face 모델: keras<3.x)
import tensorflow as tf
import keras
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import load_model  # 모델 불러오기
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(tf.__version__)
print(keras.__version__)

data = pd.read_csv('./simulation_data.csv')
data = data.copy()
data = data[data['sensor_id']==7]
data

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf

# 시간 열을 datetime으로 변환
data['time'] = pd.to_datetime(data['time'])

# 'time'을 인덱스로 설정
data = data.set_index('time')

# 필요한 칼럼만 선택
data = data[['filtered_value', 'outlier_status']]

# 데이터 정규화
scaler = MinMaxScaler()
data[['value']] = scaler.fit_transform(data[['filtered_value']])

# 시계열 데이터로 변환
def create_sequences(df, sequence_length):
    xs, ys = [], []
    for i in range(len(df) - sequence_length):
        x = df.iloc[i:i+sequence_length][['value']].values
        y = df.iloc[i+sequence_length]['outlier_status']
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

sequence_length = 288  # 모델이 기대하는 시퀀스 길이
X, y = create_sequences(data, sequence_length)

# 데이터 차원 확인
print("X shape before reshape:", X.shape)

# 차원 조정 (마지막 차원을 1로 변경)
X = X[:, :, 0:1]

print("X shape after reshape:", X.shape)

# 훈련 세트와 테스트 세트로 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# 모델 로드
model = tf.keras.models.load_model("C:\\Users\\Jsystem\\.cache\\huggingface\\hub\\models--keras-io--timeseries-anomaly-detection\\snapshots\\57126012e02dcab3a653501f12cb599c4a51db7a")
model.summary()

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# 모델 학습
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# 예측
y_pred = model.predict(X_test)

# 모델 평가
loss, accuracy = model.evaluate(X_test, y_test)
print(" ")
print("==========================")
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")




######################################################################################

## 버전 확인 (hugging face 모델: keras<3.x)
import tensorflow as tf
import keras
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import load_model  # 모델 불러오기
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(tf.__version__)
print(keras.__version__)

data = pd.read_csv('./preprocessed_data0912(symbolic).csv')

# '오후', '오전'을 AM/PM으로 변환하는 함수
def convert_korean_time_to_ampm(time_str):
    if '오전' in time_str:
        return time_str.replace('오전', 'AM')
    elif '오후' in time_str:
        return time_str.replace('오후', 'PM')
    else:
        return time_str

# 'time' 컬럼에 있는 '오전', '오후'를 AM/PM으로 변환
data['time'] = data['time'].apply(convert_korean_time_to_ampm)

# 날짜 및 시간 합치기
data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'], format='%Y-%m-%d %I:%M:%S %p')

# 불필요한 칼럼 제거
data = data.drop(columns=['test_id', 'date', 'time'])

# 칼럼순서 변경
data = data[['datetime', 'temp', 'rh', 'label']]

data['datetime'] = pd.to_datetime(data['datetime'])
data = data.set_index('datetime')

# 데이터 정규화
scaler = MinMaxScaler()
data[['temp', 'rh']] = scaler.fit_transform(data[['temp', 'rh']])

# 시계열 데이터로 변환
def create_sequences(df, sequence_length):
    xs, ys = [], []
    for i in range(len(df) - sequence_length):
        x = df.iloc[i:i+sequence_length][['temp', 'rh']].values
        y = df.iloc[i+sequence_length]['label']
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

sequence_length = 288  # 모델이 기대하는 시퀀스 길이
X, y = create_sequences(data, sequence_length)

# 현재 X shape 확인
print("X shape before reshape:", X.shape)

# 데이터 차원 조정 (마지막 차원을 1로 변경)
X = X[:, :, 0:1]

# 현재 X shape 확인
print("X shape after reshape:", X.shape)

# 훈련 세트와 테스트 세트로 나누기
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

import tensorflow as tf

model = tf.keras.models.load_model("C:\\Users\\Jsystem\\.cache\\huggingface\\hub\\models--keras-io--timeseries-anomaly-detection\\snapshots\\57126012e02dcab3a653501f12cb599c4a51db7a")
model.summary()

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 모델 학습
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# 예측
y_pred = model.predict(X_test)

# 모델 평가
loss, accuracy = model.evaluate(X_test, y_test)
print(" ")
print(" ")
print("==========================")
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")
